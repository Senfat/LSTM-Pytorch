{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentense classification_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7OvkM6JJZC",
        "outputId": "b7f469aa-c46e-4e42-b546-7c6c3fc68718"
      },
      "source": [
        "!pip install pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
            "\r\u001b[K     |███▋                            | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51kB 11.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 81kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.18.5)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCQsCRiYE3Du"
      },
      "source": [
        "import argparse\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import TransformerMixin\n",
        "import sklearn.metrics as skm\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchnlp.samplers import BucketBatchSampler\n",
        "from torchnlp.utils import collate_tensors\n",
        "from torchnlp.encoders.text import stack_and_pad_tensors\n",
        "from torchnlp.encoders.text import WhitespaceEncoder, SpacyEncoder\n",
        "from numpy import vstack\n",
        "from pandas import read_csv\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRCRCpUXFPi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b664760-81cf-47c9-af1d-22465db57fba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0iCGp2zHPji",
        "outputId": "9f9239c1-e3e9-4133-ac55-2c4f8cab6cd8"
      },
      "source": [
        "!ls drive/MyDrive/Project02"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "project02_not_full_train.csv   project02_toloka_unlabeled.csv\n",
            "project02_submission_file.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvnE3WREHe-t",
        "outputId": "f14a4b99-a389-4397-c660-e62c327986e8"
      },
      "source": [
        "df_train = pd.read_csv ('drive/MyDrive/Project02/project02_not_full_train.csv')\n",
        "\n",
        "df_test = pd.read_csv ('drive/MyDrive/Project02/project02_submission_file.csv')\n",
        "\n",
        "print (df_train)\n",
        "\n",
        "print (df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               Text            Label\n",
            "0         Вызвать такси на семь человек до Одинцово       ORDER_TAXI\n",
            "1     Самый страшный ужастик по книге Стивена Кинга       PLAY_MOVIE\n",
            "2                           Анекдот с черным юмором        READ_JOKE\n",
            "3                 Сколько я буду ехать до заправки?         NAVIGATE\n",
            "4                              Кто такой Оксимирон?             ODQA\n",
            "...                                             ...              ...\n",
            "1141        Подай такси к дому через двадцать минут       ORDER_TAXI\n",
            "1142                           Заказать стол в пабе  BOOK_RESTAURANT\n",
            "1143                           Хочу включить музыку       PLAY_MUSIC\n",
            "1144  Напомни Саше зайти к репетитору забрать книгу         REMINDER\n",
            "1145           Сколько ехать сейчас до Сухаревской?         NAVIGATE\n",
            "\n",
            "[1146 rows x 2 columns]\n",
            "                                                  Text  Label\n",
            "0                     Дождь закончится через два часа?    NaN\n",
            "1                                 Синус угла пи на два    NaN\n",
            "2                             Какая столица Австралии?    NaN\n",
            "3    Хочу поужинать в ресторане в семь часов вечера...    NaN\n",
            "4                       Уехать на такси до станции ЗИЛ    NaN\n",
            "..                                                 ...    ...\n",
            "307                        Хочу поехать домой на такси    NaN\n",
            "308                   Переведи одна тысяча рублей маме    NaN\n",
            "309                           Заплати все коммунальные    NaN\n",
            "310                                 Путь до Домодедово    NaN\n",
            "311  Забронировать место на двоих в самом дорогом р...    NaN\n",
            "\n",
            "[312 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV8Rx1jVHsj9",
        "outputId": "4db508e3-e84a-48f9-94fa-1c4fba632c97"
      },
      "source": [
        "dict_map = {'ORDER_TAXI': 1, 'PLAY_MOVIE': 2, 'READ_JOKE':3, 'NAVIGATE': 4,\n",
        "            'ODQA': 5, 'WEATHER': 6, 'COOKING': 7, 'LEGEND': 8, 'PLAY_MUSIC':9,\n",
        "            'BOOK_RESTAURANT': 10, 'REMINDER':11, 'READ_NEWS': 12, 'CALCULATOR':13,\n",
        "            'SHOPPING_LIST': 14, 'P2P_TRANSFER': 15, 'CHECK_ACCOUNT ':16, 'CHECK_ACCOUNT':16, 'CURRENCY':17,\n",
        "            'COMMUNAL_PAYMENTS':18, 'FAIL_FEEDBACK': 19, 'PLAY_RADIO':20,\n",
        "            'SBER_PRODUCTS':21, 'HARD_COMMAND':22, 'SET_ALARM':23, 'TIME':24,\n",
        "            'SET_TIMER': 25, 'CHITCHAT':26, 'CALENDAR':27}\n",
        "\n",
        "df_train['Index_Label'] = df_train.Label.map (dict_map)\n",
        "\n",
        "labels = df_train.drop_duplicates(subset='Label')\n",
        "labels['Index_Label'] = labels.Label.map (dict_map)\n",
        "print (labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  Text  ... Index_Label\n",
            "0            Вызвать такси на семь человек до Одинцово  ...           1\n",
            "1        Самый страшный ужастик по книге Стивена Кинга  ...           2\n",
            "2                              Анекдот с черным юмором  ...           3\n",
            "3                    Сколько я буду ехать до заправки?  ...           4\n",
            "4                                 Кто такой Оксимирон?  ...           5\n",
            "5                            Будет дождливо в субботу?  ...           6\n",
            "8                                Хочу пожарить курочку  ...           7\n",
            "12                                   Я закрыла сессию!  ...           8\n",
            "13             Поставь самый популярный альбом Земфиры  ...           9\n",
            "14    Закажи столик на десять человек на вечер пятницы  ...          10\n",
            "17                        Добавь напоминание на завтра  ...          11\n",
            "23                            Прочитай новости от РБК   ...          12\n",
            "24                        Пять процентов от семи сотен  ...          13\n",
            "34                                Что в списке покупке  ...          14\n",
            "35   Отправь Пете одна тысяча пятьсот рублей по ном...  ...          15\n",
            "40                          Какой баланс на моей карте  ...          16\n",
            "47                   Курс белорусского рубля к доллару  ...          17\n",
            "50                      Какая сумма счета за квартиру?  ...          18\n",
            "56                                  Блин ты че тупишь?  ...          19\n",
            "57                             Включи последнее радио   ...          20\n",
            "58              Какие бонусы начисляются на карту мир?  ...          21\n",
            "63                                    Открой настройки  ...          22\n",
            "95                       Отмени будильники на выходных  ...          23\n",
            "109                               Какое число сегодня?  ...          24\n",
            "118             Таймер на семь минут и тридцать секунд  ...          25\n",
            "200                            Хочу поговорить с тобой  ...          26\n",
            "215       Удали встречу которая на семь вечера сегодня  ...          27\n",
            "\n",
            "[27 rows x 3 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSVb6NcLt82O",
        "outputId": "1727b087-1207-4a0c-8be5-4baa07b0ef7f"
      },
      "source": [
        "lb = LabelBinarizer()\n",
        "lb.fit(labels['Index_Label'].values)\n",
        "y_train = lb.transform(df_train[\"Index_Label\"].values)\n",
        "print (y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUP4CvYkaGNt",
        "outputId": "e1ea2088-a7af-45a4-818e-1e95adc382d7"
      },
      "source": [
        "encoder = SpacyEncoder(df_train.Text.values)\n",
        "train_data = [{'text': encoder.encode(example), 'label': torch.FloatTensor(label)} for example, label in zip(df_train.Text.values, y_train)]\n",
        "test_data = [{'text': encoder.encode(example)} for example in df_test.Text.values]\n",
        "\n",
        "test_data[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': tensor([  1,   1,   1,   7, 683])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFQJaawKur-1"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_sampler = torch.utils.data.sampler.RandomSampler(train_data)\n",
        "train_batch_sampler = BucketBatchSampler(\n",
        "    train_sampler, batch_size=batch_size, drop_last=True, sort_key=lambda i: \n",
        "    train_data[i]['text'].shape[0])\n",
        "\n",
        "batches = [[train_data[i] for i in batch] for batch in train_batch_sampler]\n",
        "train_batches = [collate_tensors(batch, stack_tensors=stack_and_pad_tensors) \n",
        "for batch in batches]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kprsi8TwAwOq",
        "outputId": "2de9ace1-d4d7-4c44-8c57-c6e1e958ccc1"
      },
      "source": [
        "batch_size = 1\n",
        "\n",
        "valid_sampler = torch.utils.data.sampler.SequentialSampler(test_data)\n",
        "valid_batch_sampler = BatchSampler(\n",
        "    valid_sampler, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "batches = [[test_data[i] for i in batch] for batch in valid_batch_sampler]\n",
        "valid_batches = [collate_tensors(batch, stack_tensors=stack_and_pad_tensors) for batch in batches]\n",
        "print (valid_batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': BatchedSequences(tensor=tensor([[159,   1,  84, 683, 437,  28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[  1,   1,   1,   7, 683]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[210, 305,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  45,  714,   34,   79,   34,    8,  326,  327,   95, 1182,   34,  439]]), lengths=tensor([12]))}, {'text': BatchedSequences(tensor=tensor([[467,   7,   6,  10, 887,   1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[532,  87, 154,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  65,  693,   95, 1306,  975]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[  1,  10, 267,   7,   6]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[1]]), lengths=tensor([1]))}, {'text': BatchedSequences(tensor=tensor([[  5, 700]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[161,  56,   7, 905,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 161,  224,  912, 1152, 1511,   34,  494,   54, 1512, 1513,   28,   95,\n",
            "           96,   34,  199,   54,  629,   28]]), lengths=tensor([18]))}, {'text': BatchedSequences(tensor=tensor([[   5, 1261,    6,   84,  108,   85]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[867,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[428, 273, 529]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[161, 231, 172, 336,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[253, 415, 416, 156, 417,  95, 142, 312, 536]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[432,   1,  88,   1,  82,   1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 36,  41,   7, 133, 134]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[45, 77,  6,  7,  1,  1,  1, 10,  1]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[588,   1,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[946,   1,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  65, 1123,    7]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[142, 755, 756, 757]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[403, 122,  34,   1,  79]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[359, 114,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 48,   1, 154,   1,   1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[836,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 1,  1, 28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 210,  871,  160,  636, 1725,   28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[532,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 48,   1,   1, 242,   1,   1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 23,  85, 353,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[499, 919, 151, 152]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[110,   1,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[   1, 1474, 1326,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[576]]), lengths=tensor([1]))}, {'text': BatchedSequences(tensor=tensor([[499, 611,   7, 223, 706,  88,  34,  43]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[  45, 1434,  530,    1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[588, 175,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[1174, 1555]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[110,   1, 322]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 588,   88, 1144,    1,    1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 328,  777, 1535,    1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[110,   1, 977]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[   1,  543, 1296,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 588, 1048,  720,   82,  721, 1038, 1039, 1405, 1406]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[1097,  656,  154,  386,  334,  157]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[  29,  539,  235,   28,   95,  280, 1112,  549,   28]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[   1, 1121,   54,  504,    1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 161,  278, 1024,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[210,   1, 945]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 45, 259,  71, 154, 672,  34,  79,   7, 673,  95,   1, 334,   1,  28]]), lengths=tensor([14]))}, {'text': BatchedSequences(tensor=tensor([[ 65,  87,   7,  43, 327, 360, 361]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 40, 160, 106, 876,  34, 830,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 164, 1060,   87,  175,  153]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 854, 1123,   95,  867, 1057]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 328,  777, 1535,  104,   28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 135,  263, 1549,  558, 1744,    1,    1]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[  1,   1,   1,   1, 154, 741,   7,   1]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[ 532,  316, 1393]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[245, 213, 477, 102,   1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[1496,  562,   10,  119]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[631,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[161,   1,  28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[171,   1,  28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[   1, 1061,  167,  292,  307, 1452]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 129, 1541,   28,   95,   96,  277,   28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 839,  431,   24,  844, 1044,   95,  937, 1057]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[161, 921,   7, 985,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 110, 1266,  389]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[982,   1, 983]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 171,  334,    1, 1514,   28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 65, 554, 627, 628, 494,  54, 629]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 1,  1, 34,  1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  1, 963]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[1486,   53,  263,   56]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[499,  87,   7, 753, 315, 901, 902]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 45, 591, 316, 530, 589]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[379, 380, 944,  34, 824,  28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[217, 613,   1,   1, 482, 144, 145, 146, 147, 223, 384]]), lengths=tensor([11]))}, {'text': BatchedSequences(tensor=tensor([[ 45, 387,   1, 235]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 65,   7,  39, 758,   7,  72]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[875, 272, 342,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[171, 932,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[432, 593, 178, 160, 106, 455,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 23, 231,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1553,   87,  154,    1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1, 1, 1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 588,  530, 1219,  157]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[359,  53, 504,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[217,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 236, 1100,    7,   88, 1748]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[   1, 1022]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 65, 562,   7,  72]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  1,   1, 334, 157]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  1, 558,   7, 103,   1, 636, 620, 167,  28]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[171,   1,  28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[  91, 1281, 1674]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[492,   6,   7, 439, 326, 327, 115, 818]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[ 839,  431,   24,  844, 1044,   95,   53, 1152,   34, 1153]]), lengths=tensor([10]))}, {'text': BatchedSequences(tensor=tensor([[  1, 530,  81, 833]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[276]]), lengths=tensor([1]))}, {'text': BatchedSequences(tensor=tensor([[171, 172,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[467,  34, 256, 141,   7, 782]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 40, 131,  37,   7,  39]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[135,   1,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[390, 525, 884,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[110,   1, 530,   1,   1,   1,  34,   1]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[ 1, 92, 93,  1, 28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[171, 947,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[135, 131,  88,   1,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[161, 231, 932,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 161,  491,  287,    7, 1261, 1262,   28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[1718,  231,    1,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 32, 106, 780,  88,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 110,  381, 1093,  301]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 499,  758,    7, 1001]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[135, 278,  34, 297,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[217, 650,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[  1,   1, 999, 353,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 23, 353, 833,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[578,   7, 207, 579]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 161, 1193,   34,    1,   28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[  1, 234]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 280, 1184,    1,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1, 1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[   1,  185,  612, 1279,    7, 1190,   34,  512,  513,  223]]), lengths=tensor([10]))}, {'text': BatchedSequences(tensor=tensor([[ 328,  231, 1514,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[946, 985, 107,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1338, 1339]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[1, 1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[12,  1,  9,  7,  1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[499,  87]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[110, 322, 115, 430]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[161, 921,  34,   1,   1, 849,  28,  95,  96,  34,   1,  28]]), lengths=tensor([12]))}, {'text': BatchedSequences(tensor=tensor([[135, 581,  28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[1047,  103,    1,  167,    1,    1,   15,  168,    1]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[257, 258,   1,  34,   8, 103, 579]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 588,  131, 1075,  235,   34,   43]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 65,  66, 956,  68,   1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[91, 92, 93,  1,  1, 28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[341,   1,  34,  79]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[210, 871,  88,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  61,  378, 1570,   34,   67, 1571,   34,   72,  327,   95,    1, 1182,\n",
            "           34,  636,  327]]), lengths=tensor([15]))}, {'text': BatchedSequences(tensor=tensor([[532,  87, 154,   1,  95, 829, 154,   1]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[ 638,    1,  612,    1,    1,   34, 1130,  549]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[  1, 963]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[236,   1,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[  1, 178, 681,  93, 136]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[217, 336, 990]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[542, 543, 544,   7, 922,  28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[1101,    1,    1,    7,  683,  437,  340]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[   1, 1071, 1072]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[210,   1,   1,   1,  15,   1,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 492,  139,  140,   15, 1537,    1,    1,  101,   72]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[110,   1,   7,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1315,  613,    1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 135,  336,  337,   95,  307,  833,    7,  292, 1377]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[135,  93, 136,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  1, 336]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 161,   20, 1671,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 1, 71,  7, 72,  9, 34,  1,  1]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[ 91, 181]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[1160,   93,  272,    1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[432,   1,   1, 326]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 23, 409,  24,   1,  34,   1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[379, 106, 380,   1,  34, 830,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[135, 689,  82,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[403, 260,   7,   8, 326,  34,   1]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 23, 278,  28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[428, 431, 530,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[171,   1,  28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[1311,  309,  360, 1236]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  23,  406,  353,    6,   10, 1546, 1200,   28]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[45, 56, 53, 54, 55,  1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[65,  1,  1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[142, 235]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[210, 871,  34,   1,  54,   1,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[110, 430]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[161, 363,   1,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[631,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 36, 131, 963]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[  45,    1,  198, 1434,  530,    1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 946,    1, 1514,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 328, 1308,  263,    1,   28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[  1, 106,  88, 157, 299,   1,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[1, 1, 1, 1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 492,  122,   34, 1653,    1,  567,  841]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[1, 1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[236,   1,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[  1, 178, 681,  93, 136,  95, 588, 131,   1, 680]]), lengths=tensor([10]))}, {'text': BatchedSequences(tensor=tensor([[161, 231, 842,  56]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1383,  132,    7,   39]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 36,  41, 132, 921, 570,  24, 329]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[423,   6,  34, 256]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[217, 336, 905]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[110,   1,   1, 879]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 29,  30,  31,  28,  95, 374,  92, 790,   1, 680,  28]]), lengths=tensor([11]))}, {'text': BatchedSequences(tensor=tensor([[236, 114,  34, 450,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 40,  41, 132,   7, 362]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 161,   92, 1518,  272,   15, 1519,   28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[217, 336]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 23, 279, 192,   1,   1,  28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 328,  349,    1,   28,   95,   96, 1467,   28]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[135, 689]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[  1,  75,   1,  82, 207, 579]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[  1,   1, 281,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1237,   93,    1,   28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 65, 688, 154,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[  1,   1, 198,   7, 112]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[135, 204,  10, 104, 105,  28,  95,  96,  10, 104,   1,  28]]), lengths=tensor([12]))}, {'text': BatchedSequences(tensor=tensor([[ 45, 591,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[  1, 114]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[158, 178,  53, 231, 172, 335,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 588, 1570,    1,   84,   43, 1598]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 135,  689,   10, 1087,    1,   28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[1311,    1, 1494,  918,    1,  176]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[  61,  187, 1414, 1238,    1,  759]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[   1,   20,    1,    1,    1, 1096]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 492,    6, 1079,    1,   10,    1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[135, 336, 337]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[171, 842, 186,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[532, 791,   7,   1,  34, 567,  34,  11]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[ 61,  88,   1,  34,   1, 681, 131,   1,  28]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[ 328, 1514,   28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[   1,    7,    1,   34,  125,    7, 1347]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[  1,   1, 167,   7, 239,  15, 168,   1]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[1486,   53,   56]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[328, 283,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 70, 139, 140,  10,  99,   1, 178, 101, 148]]), lengths=tensor([9]))}, {'text': BatchedSequences(tensor=tensor([[217,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[236, 114,  34,   1,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[1]]), lengths=tensor([1]))}, {'text': BatchedSequences(tensor=tensor([[ 23, 595,   1,   1,  28, 103,  95,  96,   1,  28]]), lengths=tensor([10]))}, {'text': BatchedSequences(tensor=tensor([[ 266, 1744,    1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[91,  1, 88]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[   1,    1, 1716,  178,    1,    1,   28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 36,   1, 488]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 661,  114,  389,   95,   96,  150, 1277,  235]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[  1, 131,  20, 556,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 1,  1,  1, 28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[217, 336,  20,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 70, 700,  10,   1, 893, 101, 395, 102]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[110,   1, 431, 530, 314]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[161, 921,  34,   1,   1, 849,  28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[ 45, 591,   1,   7,   1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[328, 349,   1,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 23,  24,  25,  26,   7, 486,  10, 267]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[110, 554,   1,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 135,   92,   93, 1100,   28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 217, 1274,    1,    1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 171,  641, 1684,    1, 1260,    1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[  12, 1461,    1, 1317]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[432,   1,   1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[161, 231, 928, 272,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[875, 107,  28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 588,  719, 1494,  475,  641, 1495,  720,   82,  721,  315, 1407,  224,\n",
            "         1039]]), lengths=tensor([13]))}, {'text': BatchedSequences(tensor=tensor([[1160,   93,  272,    1,   28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[  61,  378,    1,   71,   34, 1322,   79]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[110, 516, 147,   1, 389]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[  29,  313,    7,  920,   28,   95,  280, 1112,  549,  292, 1152,   28]]), lengths=tensor([12]))}, {'text': BatchedSequences(tensor=tensor([[110, 185,   1,   1,   1]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[  91,  131,  264, 1434,  334,   28,   95,   96,   88,   28]]), lengths=tensor([10]))}, {'text': BatchedSequences(tensor=tensor([[  91,  969,    7,  407, 1291,  409]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[  32,  106, 1352, 1001,   28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 121,  656,  154, 1201,    9,   34, 1202,   95,  738,  154,    1]]), lengths=tensor([11]))}, {'text': BatchedSequences(tensor=tensor([[588,   1, 315, 918, 160, 424, 817, 175, 176, 154, 177]]), lengths=tensor([11]))}, {'text': BatchedSequences(tensor=tensor([[161, 363, 693,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[946,   1, 160,  33,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 110,  381, 1093,  185,  216]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[161,   7, 246, 912]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1]]), lengths=tensor([1]))}, {'text': BatchedSequences(tensor=tensor([[161, 263, 264,   7, 919, 483]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[1]]), lengths=tensor([1]))}, {'text': BatchedSequences(tensor=tensor([[245, 213, 965, 516,  72, 167]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[161,  24,   1, 641, 844,  28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 280, 1112,  549,    1,    1, 1743,    1,   28]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[ 65,   1, 316]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 328,  846, 1686,  656,   95,   96,    1,   28]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[467,   7, 782,  34, 141]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 161,   92,  309,  334,  213, 1398]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[   1, 1605]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[110,  68,   1,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[110, 464, 530,   1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 91, 969,   7, 477, 334]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[   1,  118,  115,    1,    1, 1061]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[631,   1]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[ 588,  131,  175, 1497]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[1117,  758,    7,  439,  326,   95,  829,    7,  439,  326,  108,   85]]), lengths=tensor([12]))}, {'text': BatchedSequences(tensor=tensor([[  36, 1043,    1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 210,    1,    1,    7,    1,  213, 1325,  483]]), lengths=tensor([8]))}, {'text': BatchedSequences(tensor=tensor([[23,  1, 34,  1, 15,  1, 28]]), lengths=tensor([7]))}, {'text': BatchedSequences(tensor=tensor([[  65, 1260, 1584,  688]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 110, 1015,  339]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 65, 235,   7,   1, 516, 683]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 1,  1, 28]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[65,  1,  1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[631, 116]]), lengths=tensor([2]))}, {'text': BatchedSequences(tensor=tensor([[164,   1,  87, 175, 176]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[236, 114, 224, 436,  28]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[328, 366, 101,  28]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 492,  139,  140,   10,  104, 1423]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[428, 431, 530,   1,  95]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[ 5,  6, 10,  1]]), lengths=tensor([4]))}, {'text': BatchedSequences(tensor=tensor([[ 110,    1,    1,    1, 1081,    1]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[ 236, 1192,    1,  154,    1,   28]]), lengths=tensor([6]))}, {'text': BatchedSequences(tensor=tensor([[  45, 1242,  817,    7,    6]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[1047,  144,  145,  167,  704]]), lengths=tensor([5]))}, {'text': BatchedSequences(tensor=tensor([[  1, 617, 929]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 1, 10,  1]]), lengths=tensor([3]))}, {'text': BatchedSequences(tensor=tensor([[ 403,  260,    7,  672,   34, 1653,    1,   79]]), lengths=tensor([8]))}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4QhRNhtwabP"
      },
      "source": [
        "def read_word_vectors(vectors_path):\n",
        "    print(\"Reading word vectors... \")\n",
        "    with h5py.File(vectors_path, 'r') as h5_file:\n",
        "        vectors = h5_file['matrix'][...]\n",
        "        words = {w.decode('utf8'): i for i, w in enumerate(h5_file['words'][...])}  # convert list to dict\n",
        "    embed_dim = vectors.shape[1]\n",
        "    return vectors, words, embed_dim\n",
        "\n",
        "\n",
        "def get_lookup_table(word_to_id, word_vectors_path):\n",
        "    voc_size = len(word_to_id)\n",
        "    vectors, words, embed_dim = read_word_vectors(word_vectors_path)\n",
        "    # Create lookup table\n",
        "    # Words not found in embeddings index will be all-zeros.\n",
        "    lookup_table = np.zeros((voc_size, embed_dim), dtype=np.float32)\n",
        "    num_words_without_vector = 0\n",
        "    for word, i in word_to_id.items():\n",
        "        if word in words:\n",
        "            lookup_table[i] = vectors[words[word]]\n",
        "        else:\n",
        "            num_words_without_vector += 1\n",
        "    print(\n",
        "        \"{} unique tokens, {} without vectors\".format(voc_size, num_words_without_vector))\n",
        "    return lookup_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5gmrfqXw4Jq"
      },
      "source": [
        "def evaluate(batches, model, criterion, device, best_f1=False):\n",
        "    \"\"\"\n",
        "    Evaluation, return accuracy and loss\n",
        "    \"\"\"\n",
        "    total_loss = 0.\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    with torch.no_grad():\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = batch['text'].tensor.to(device), batch['label'].tensor.to(device)\n",
        "            output = model(x_batch)\n",
        "            total_loss += criterion(output, y_batch)\n",
        "            y_pred.extend(sigmoid(output).cpu().numpy())  # don't forget to execute sigmoid function on logits\n",
        "            y_true.extend(y_batch.cpu().numpy())\n",
        "    y_true = np.asarray(y_true, dtype=np.uint8)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    # finding the best threshold with highest f1 score\n",
        "    if best_f1:\n",
        "        thresholds = np.linspace(0.2, 1, 80)\n",
        "        f1s = [skm.f1_score(y_true, np.array(y_pred > thr, dtype=np.uint8), average='samples') for thr in thresholds]\n",
        "        best_index = np.argmax(f1s)\n",
        "        return {'f1': f1s[best_index], 'threshold': thresholds[best_index], 'loss': total_loss / len(batches)}\n",
        "    else:\n",
        "        f1 = skm.f1_score(y_true, np.array(y_pred >= 0.5, dtype=np.uint8), average='samples')\n",
        "        return {'f1': f1, 'loss': total_loss / len(batches)}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc_k0P4Lw6sl"
      },
      "source": [
        "class EmbeddingFromFile(nn.Embedding):\n",
        "    def __init__(self,\n",
        "                 word_to_id,\n",
        "                 word_vectors_path):\n",
        "        lookup_table = get_lookup_table(word_to_id, word_vectors_path)\n",
        "        super().__init__(\n",
        "            len(word_to_id),\n",
        "            lookup_table.shape[1],\n",
        "            _weight=torch.from_numpy(lookup_table)\n",
        "        )\n",
        "\n",
        "\n",
        "class ModuleParallel(nn.Module):\n",
        "    \"\"\"\n",
        "    Execute multiple modules on the same input and concatenate the results\n",
        "    \"\"\"\n",
        "    def __init__(self, modules: list, axis=1):\n",
        "        super().__init__()\n",
        "        self.modules_ = nn.ModuleList(modules)\n",
        "        self.axis = axis\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([m(input) for m in self.modules_], self.axis)\n",
        "\n",
        "\n",
        "class GlobalMaxPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.max(2)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAKmlQ2qw9Sk"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, query_dim):\n",
        "    super(Attention, self).__init__()\n",
        "    self.scale = 1. / math.sqrt(query_dim)\n",
        "\n",
        "  def forward(self, query, keys, values):\n",
        "    # Query = [BxQ]\n",
        "    # Keys = [TxBxK]\n",
        "    # Values = [TxBxV]\n",
        "    # Outputs = a:[TxB], lin_comb:[BxV]\n",
        "\n",
        "    # Here we assume q_dim == k_dim (dot product attention)\n",
        "\n",
        "    query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n",
        "    keys = keys.transpose(1,2) # [TxBxK] -> [BxKxT]\n",
        "    energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
        "    energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n",
        "\n",
        "    linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
        "    return energy, linear_combination\n",
        "\n",
        "\n",
        "class LSTMTextClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN-based text classifier\n",
        "\n",
        "    It can be used for both multi-class and multi-label classification problem,\n",
        "     because loss is not specified\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 word_to_id,\n",
        "                 use_pretrained_word_vectors=True,\n",
        "                 word_vectors_path=\"data/glove.42B.300d.txt\",\n",
        "                 trainable_word_vectors=True,\n",
        "                 embed_dim=100,\n",
        "                 num_layers=1,\n",
        "                 bidirectional=False,\n",
        "                 self_attention=False,\n",
        "                 dense_sizes=(1000,),\n",
        "                 dense_dropout=0.6,\n",
        "                 rnn_dropout=0.0,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        :param num_classes: number of outputs (classes)\n",
        "        :param word_to_id: dictionary used to compose lookup table\n",
        "        :param use_pretrained_word_vectors: whether to use pre-trained word vectors\n",
        "        :param word_vectors_path: path to word vectors file (should be in compatible format)\n",
        "        :param trainable_word_vectors: whether to train (change) vectors\n",
        "        :param embed_dim: embedding dimensionality in case of `use_pretrained_word_vectors=False`\n",
        "        :param filters: number of filters (output channels) for each kernel size of the 1st CNN layer\n",
        "        :param kernel_sizes: kernel sizes of the 1st CNN layer\n",
        "        :param pooling_dropout: dropout coefficient after pooling layer\n",
        "        :param dense_sizes: sizes of fully-connected layers\n",
        "        :param dense_dropout: dropout coefficient after each fully-connected layer\n",
        "        :param kwargs: ignored arguments\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        vocabulary_size = len(word_to_id)\n",
        "        if use_pretrained_word_vectors:\n",
        "            self.embed = EmbeddingFromFile(word_to_id, word_vectors_path)\n",
        "        else:\n",
        "            self.embed = nn.Embedding(vocabulary_size, embed_dim)\n",
        "\n",
        "        if not trainable_word_vectors:\n",
        "            self.embed.weight.requires_grad = False\n",
        "\n",
        "        self.bi = bidirectional\n",
        "        self.rnn = nn.LSTM(input_size=self.embed.embedding_dim, hidden_size=300,\n",
        "                           bidirectional=bidirectional, num_layers=num_layers,\n",
        "                           batch_first=True, dropout=rnn_dropout)\n",
        "        self.self_attention = self_attention\n",
        "        attention_dim = 300 if not self.bi else 2 * 300\n",
        "        if self_attention:\n",
        "            self.attention = SelfAttention(attention_dim)\n",
        "\n",
        "        self.fcs = nn.Sequential(OrderedDict(chain(*[\n",
        "            [\n",
        "                ('fc{}'.format(i), nn.Linear(attention_dim, dense_sizes[i])),\n",
        "                ('fc{}_bn'.format(i), nn.BatchNorm1d(dense_sizes[i])),\n",
        "                ('fc{}_relu'.format(i), nn.ReLU(inplace=True)),\n",
        "                ('fc{}_dp'.format(i), nn.Dropout(dense_dropout))\n",
        "            ] for i in range(len(dense_sizes))\n",
        "        ])))\n",
        "        self.fc_last = nn.Linear(dense_sizes[-1], num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv1d takes in (batch, channels, seq_len), but raw embedded is (batch, seq_len, channels)\n",
        "        x = self.embed(x)  # .permute(0, 2, 1)\n",
        "        outputs, hidden = self.rnn(x)\n",
        "\n",
        "        if self.self_attention:\n",
        "            # attention calculation\n",
        "            if self.bi:\n",
        "                hidden = torch.cat([hidden[0][-1], hidden[0][-2]], dim=1)\n",
        "            else:\n",
        "                hidden = hidden[0][-1]\n",
        "            energy, x = self.attention(hidden, outputs, outputs)\n",
        "        else:\n",
        "            x = outputs[:, -1, :]\n",
        "\n",
        "        x = self.fcs(x)\n",
        "        x = self.fc_last(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69adKriUxBzu",
        "outputId": "82d22821-67fb-4374-df14-d8350b4dacd6"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "model = LSTMTextClassifier(\n",
        "        y_train.shape[1],\n",
        "        encoder.token_to_index,\n",
        "        word_vectors_path= '/content/drive/My Drive/cc.ru.300.1M.h5',\n",
        "        kernel_sizes=[1, 2],\n",
        "        filters=[600, 600],\n",
        "        dense_sizes=[1200],\n",
        "        trainable_word_vectors=True\n",
        "    ).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading word vectors... \n",
            "1758 unique tokens, 90 without vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmg3YAaqyVl4"
      },
      "source": [
        "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
        "                        lr=0.001, weight_decay=0)\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='sum')   # sigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMPfyeny9HkE",
        "outputId": "52e53dee-f233-4e22-92c4-c3720a92b8d9"
      },
      "source": [
        "for epoch in range(70):\n",
        "    model.train()\n",
        "    \n",
        "    for batch in train_batches:\n",
        "        x_batch, y_batch = batch['text'].tensor.to(device), batch['label'].tensor.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = criterion(model(x_batch), y_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        # clipping gradients\n",
        "        torch.nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad], 1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    train_metrics = evaluate(train_batches, model, criterion, device)\n",
        "\n",
        "    print('Epoch {:3}, {}'\n",
        "                .format(epoch + 1, ' '.join(['train_{}: {:<6.4f}'.format(k, v) for k, v in train_metrics.items()]),\n",
        "                      ))\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1, train_f1: 0.3402 train_loss: 361.8914\n",
            "Epoch   2, train_f1: 0.7479 train_loss: 43.4370\n",
            "Epoch   3, train_f1: 0.8595 train_loss: 16.3594\n",
            "Epoch   4, train_f1: 0.9396 train_loss: 6.8438\n",
            "Epoch   5, train_f1: 0.9786 train_loss: 4.0633\n",
            "Epoch   6, train_f1: 0.9905 train_loss: 1.9396\n",
            "Epoch   7, train_f1: 0.9818 train_loss: 1.8593\n",
            "Epoch   8, train_f1: 0.9970 train_loss: 0.4743\n",
            "Epoch   9, train_f1: 0.9955 train_loss: 0.6888\n",
            "Epoch  10, train_f1: 0.9991 train_loss: 0.3047\n",
            "Epoch  11, train_f1: 0.9973 train_loss: 0.3675\n",
            "Epoch  12, train_f1: 0.9991 train_loss: 0.1157\n",
            "Epoch  13, train_f1: 0.9982 train_loss: 0.3325\n",
            "Epoch  14, train_f1: 1.0000 train_loss: 0.0722\n",
            "Epoch  15, train_f1: 1.0000 train_loss: 0.0678\n",
            "Epoch  16, train_f1: 1.0000 train_loss: 0.0598\n",
            "Epoch  17, train_f1: 1.0000 train_loss: 0.0576\n",
            "Epoch  18, train_f1: 1.0000 train_loss: 0.0578\n",
            "Epoch  19, train_f1: 1.0000 train_loss: 0.0591\n",
            "Epoch  20, train_f1: 1.0000 train_loss: 0.0625\n",
            "Epoch  21, train_f1: 1.0000 train_loss: 0.0648\n",
            "Epoch  22, train_f1: 1.0000 train_loss: 0.1335\n",
            "Epoch  23, train_f1: 1.0000 train_loss: 0.1417\n",
            "Epoch  24, train_f1: 1.0000 train_loss: 0.1274\n",
            "Epoch  25, train_f1: 1.0000 train_loss: 0.1192\n",
            "Epoch  26, train_f1: 1.0000 train_loss: 0.1161\n",
            "Epoch  27, train_f1: 1.0000 train_loss: 0.1166\n",
            "Epoch  28, train_f1: 1.0000 train_loss: 0.1344\n",
            "Epoch  29, train_f1: 1.0000 train_loss: 0.1318\n",
            "Epoch  30, train_f1: 1.0000 train_loss: 0.1382\n",
            "Epoch  31, train_f1: 1.0000 train_loss: 0.1371\n",
            "Epoch  32, train_f1: 1.0000 train_loss: 0.1349\n",
            "Epoch  33, train_f1: 1.0000 train_loss: 0.1326\n",
            "Epoch  34, train_f1: 1.0000 train_loss: 0.1303\n",
            "Epoch  35, train_f1: 1.0000 train_loss: 0.1273\n",
            "Epoch  36, train_f1: 1.0000 train_loss: 0.1163\n",
            "Epoch  37, train_f1: 1.0000 train_loss: 0.1099\n",
            "Epoch  38, train_f1: 1.0000 train_loss: 0.1029\n",
            "Epoch  39, train_f1: 1.0000 train_loss: 0.1049\n",
            "Epoch  40, train_f1: 1.0000 train_loss: 0.1048\n",
            "Epoch  41, train_f1: 1.0000 train_loss: 0.1079\n",
            "Epoch  42, train_f1: 1.0000 train_loss: 0.1070\n",
            "Epoch  43, train_f1: 1.0000 train_loss: 0.1040\n",
            "Epoch  44, train_f1: 1.0000 train_loss: 0.1024\n",
            "Epoch  45, train_f1: 1.0000 train_loss: 0.0932\n",
            "Epoch  46, train_f1: 1.0000 train_loss: 0.0957\n",
            "Epoch  47, train_f1: 1.0000 train_loss: 0.0939\n",
            "Epoch  48, train_f1: 1.0000 train_loss: 0.0933\n",
            "Epoch  49, train_f1: 1.0000 train_loss: 0.0940\n",
            "Epoch  50, train_f1: 1.0000 train_loss: 0.0884\n",
            "Epoch  51, train_f1: 1.0000 train_loss: 0.0856\n",
            "Epoch  52, train_f1: 1.0000 train_loss: 0.0866\n",
            "Epoch  53, train_f1: 1.0000 train_loss: 0.0858\n",
            "Epoch  54, train_f1: 1.0000 train_loss: 0.0845\n",
            "Epoch  55, train_f1: 1.0000 train_loss: 0.0824\n",
            "Epoch  56, train_f1: 1.0000 train_loss: 0.0836\n",
            "Epoch  57, train_f1: 1.0000 train_loss: 0.0846\n",
            "Epoch  58, train_f1: 1.0000 train_loss: 0.0812\n",
            "Epoch  59, train_f1: 1.0000 train_loss: 0.0833\n",
            "Epoch  60, train_f1: 1.0000 train_loss: 0.0821\n",
            "Epoch  61, train_f1: 1.0000 train_loss: 0.0869\n",
            "Epoch  62, train_f1: 1.0000 train_loss: 0.0891\n",
            "Epoch  63, train_f1: 1.0000 train_loss: 0.0879\n",
            "Epoch  64, train_f1: 1.0000 train_loss: 0.0906\n",
            "Epoch  65, train_f1: 1.0000 train_loss: 0.0897\n",
            "Epoch  66, train_f1: 1.0000 train_loss: 0.0897\n",
            "Epoch  67, train_f1: 1.0000 train_loss: 0.0893\n",
            "Epoch  68, train_f1: 1.0000 train_loss: 0.0887\n",
            "Epoch  69, train_f1: 1.0000 train_loss: 0.0883\n",
            "Epoch  70, train_f1: 1.0000 train_loss: 0.0838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMTextClassifier(\n",
              "  (embed): EmbeddingFromFile(1758, 300)\n",
              "  (rnn): LSTM(300, 300, batch_first=True)\n",
              "  (fcs): Sequential(\n",
              "    (fc0): Linear(in_features=300, out_features=1200, bias=True)\n",
              "    (fc0_bn): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (fc0_relu): ReLU(inplace=True)\n",
              "    (fc0_dp): Dropout(p=0.6, inplace=False)\n",
              "  )\n",
              "  (fc_last): Linear(in_features=1200, out_features=27, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqJpYE3glItW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIRZHnTEc3xs"
      },
      "source": [
        "def predict(batch, model):\n",
        "    y_pred = []\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "          for i in range (len (batch)):\n",
        "            x_batch = batch[i]['text'].tensor.to(device)\n",
        "            output = model(x_batch)\n",
        "            sigmoid = nn.Sigmoid()\n",
        "            y_pred.extend(sigmoid(output).cpu().numpy())\n",
        "            pred_list.append (y_pred)\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24JV2SmsEcqN"
      },
      "source": [
        "pred_list = predict(valid_batches, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3wg2C9fdZsv"
      },
      "source": [
        "index_list = []\n",
        "for array in pred_list:\n",
        "  a = list (array)\n",
        "  index = a.index (max(a))\n",
        "  index_list.append (index+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmPp-mbo79jU",
        "outputId": "b3af2fae-0da7-48c0-8b99-72fe2690f26e"
      },
      "source": [
        "print (index_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 22, 5, 10, 1, 11, 9, 1, 23, 1, 7, 9, 1, 9, 2, 8, 2, 11, 6, 1, 11, 6, 25, 9, 10, 12, 9, 7, 5, 6, 9, 9, 24, 14, 9, 8, 21, 24, 2, 11, 22, 7, 11, 4, 9, 22, 11, 10, 5, 2, 8, 6, 10, 11, 6, 11, 25, 4, 21, 2, 9, 18, 22, 12, 8, 8, 17, 8, 2, 6, 20, 23, 8, 9, 14, 11, 7, 11, 9, 6, 9, 20, 23, 23, 8, 6, 8, 11, 9, 11, 3, 9, 27, 5, 22, 2, 21, 8, 17, 1, 2, 1, 8, 8, 1, 6, 8, 5, 2, 8, 8, 8, 8, 9, 8, 6, 9, 23, 8, 7, 24, 24, 23, 12, 9, 6, 9, 9, 8, 6, 5, 9, 2, 11, 7, 6, 8, 15, 23, 11, 9, 8, 10, 6, 10, 11, 9, 11, 9, 8, 7, 12, 9, 26, 18, 1, 2, 9, 7, 8, 7, 8, 10, 5, 8, 23, 18, 6, 4, 10, 8, 2, 8, 11, 1, 7, 9, 20, 6, 7, 5, 12, 11, 2, 8, 8, 6, 9, 10, 9, 9, 8, 7, 6, 6, 1, 7, 9, 8, 12, 6, 8, 7, 5, 5, 8, 11, 8, 8, 9, 2, 4, 9, 12, 8, 11, 4, 11, 21, 9, 1, 7, 8, 10, 8, 8, 10, 15, 7, 5, 1, 9, 12, 23, 8, 7, 5, 21, 4, 20, 8, 5, 7, 1, 2, 6, 2, 5, 4, 9, 8, 7, 19, 5, 9, 8, 6, 11, 8, 10, 20, 5, 9, 2, 18, 6, 10, 11, 5, 6, 9, 9, 23, 2, 23, 18, 2, 5, 9, 4, 1, 27, 22, 9, 2, 18, 13, 12, 11, 23, 11, 16, 18, 9, 9, 13, 5, 9, 12, 11, 12, 8, 1, 2, 1, 9, 7, 1, 15, 18, 4, 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTHDDz9S-a3H",
        "outputId": "23d0dab8-0354-4e43-ef0c-d7e1f4864fb6"
      },
      "source": [
        "df_answer = df_test.copy()\n",
        "df_answer ['Label'] = index_list\n",
        "index_to_label = {1: 'ORDER_TAXI', 2: 'PLAY_MOVIE', 3:'READ_JOKE', 4: 'NAVIGATE',\n",
        "            5: 'ODQA', 6: 'WEATHER', 7: 'COOKING', 8: 'LEGEND', 9:'PLAY_MUSIC',\n",
        "            10: 'BOOK_RESTAURANT', 11:'REMINDER', 12:'READ_NEWS', 13:'CALCULATOR',\n",
        "            14:'SHOPPING_LIST', 15:'P2P_TRANSFER', 16:'CHECK_ACCOUNT ', 16:'CHECK_ACCOUNT', 17:'CURRENCY',\n",
        "            18:'COMMUNAL_PAYMENTS', 19:'FAIL_FEEDBACK', 20:'PLAY_RADIO',\n",
        "            21:'SBER_PRODUCTS', 22:'HARD_COMMAND', 23:'SET_ALARM', 24:'TIME',\n",
        "            25:'SET_TIMER', 26:'CHITCHAT', 27:'CALENDAR'}\n",
        "\n",
        "df_answer['Label'] = df_answer.Label.map (index_to_label)\n",
        "print(df_answer)\n",
        "df_answer.to_csv ('vladislav.alferov_project02.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  Text              Label\n",
            "0                     Дождь закончится через два часа?            WEATHER\n",
            "1                                 Синус угла пи на два       HARD_COMMAND\n",
            "2                             Какая столица Австралии?               ODQA\n",
            "3    Хочу поужинать в ресторане в семь часов вечера...    BOOK_RESTAURANT\n",
            "4                       Уехать на такси до станции ЗИЛ         ORDER_TAXI\n",
            "..                                                 ...                ...\n",
            "307                        Хочу поехать домой на такси         ORDER_TAXI\n",
            "308                   Переведи одна тысяча рублей маме       P2P_TRANSFER\n",
            "309                           Заплати все коммунальные  COMMUNAL_PAYMENTS\n",
            "310                                 Путь до Домодедово           NAVIGATE\n",
            "311  Забронировать место на двоих в самом дорогом р...    BOOK_RESTAURANT\n",
            "\n",
            "[312 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMGa4VPn-74N",
        "outputId": "c680df38-5d13-47d6-b12f-ceb029c8e89b"
      },
      "source": [
        "!curl --user upload:newprolabupload -T vladislav.alferov_project02.csv 'http://de.newprolab.com/upload/' -vvv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*   Trying 85.192.32.238...\n",
            "* TCP_NODELAY set\n",
            "* Connected to de.newprolab.com (85.192.32.238) port 80 (#0)\n",
            "* Server auth using Basic with user 'upload'\n",
            "> PUT /upload/vladislav.alferov_project02.csv HTTP/1.1\r\n",
            "> Host: de.newprolab.com\r\n",
            "> Authorization: Basic dXBsb2FkOm5ld3Byb2xhYnVwbG9hZA==\r\n",
            "> User-Agent: curl/7.58.0\r\n",
            "> Accept: */*\r\n",
            "> Content-Length: 21467\r\n",
            "> Expect: 100-continue\r\n",
            "> \n",
            "< HTTP/1.1 100 Continue\n",
            "* We are completely uploaded and fine\n",
            "< HTTP/1.1 204 No Content\n",
            "< Server: nginx/1.10.3 (Ubuntu)\n",
            "< Date: Sun, 29 Nov 2020 16:52:52 GMT\n",
            "< Connection: keep-alive\n",
            "< \n",
            "* Connection #0 to host de.newprolab.com left intact\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVn36wCNVJam"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}